\documentclass[]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{capt-of}
\usepackage{listings}
\usepackage{siunitx}
\usepackage[section]{placeins}
\usepackage{float}



% Oppgavenummerering %
\renewcommand\thesection{Problem \arabic{section}}
\renewcommand\thesubsection{\alph{subsection})}

% Bevis
\newcommand\TombStone{\rule{.5em}{.5em}}
\renewcommand\qedsymbol{\TombStone}
\renewcommand{\proofname}{Bevis.} % Norske bevis

\title{TTK4215 â€“ Adaptive Assignment 4}
\author{Sigurd Totland | MTTK}

\begin{document}
\maketitle

\section{4.9 I\&S (c,d,e)}
\setcounter{subsection}{2}
\subsection{}
We define the MSD system as in assignment 4, having
\begin{equation}\begin{aligned}
\dot x = Ax + bu,
\end{aligned}\end{equation}
where
\begin{equation}\begin{aligned}
A =
\begin{bmatrix}
0 & 1 \\
-\frac{k}{m} & -\frac{\beta}{m} \\
\end{bmatrix}, \quad \text{and} \quad
b =
\begin{bmatrix}
0\\
\frac{1}{m}\\
\end{bmatrix}.
\end{aligned}\end{equation}
We use the pure least-squares adaptive law from table 4.3 this time. That is,
\begin{equation}\begin{aligned}
\label{eq:pure_least_squares}
\dot \theta &= P \epsilon \phi, \quad \text{and} \\
\dot P &= -P \frac{\phi \phi^\top}{m^2}P, \quad P(0) = P_0
\end{aligned}\end{equation}
\subsection{}
As with the state, we use forward euler to simulate the dynamics in \eqref{eq:pure_least_squares}.
After tuning, we decide on the initial $P$ matrix
\begin{equation}\begin{aligned}
P_0 = 10I_3,
\end{aligned}\end{equation}
where $I_3$ denotes the $3 \times 3$-identity-matrix, and input
\begin{equation}\begin{aligned}
u(t) = 5 \sin(t)
\end{aligned}\end{equation}
as this yields sufficiently fast parameter convergence. Our initial guess without the weights before the identity matrix and the sine, the estimates converge very slowly to the real value and ends up with a significant error in the end. The good convergence is plotted in figure \ref{fig:pure}.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth, trim={5cm 9cm 5cm 9cm, trim}]{pure}
\caption{Parameter convergence with pure least squares adaptive law}
\label{fig:pure}
\end{figure}

\subsection{}
Letting the mass change after some time by defining it
\begin{equation}\begin{aligned}
m(t) = 20(2 - e^{-0.01(t-20)})
\end{aligned}\end{equation}
for $t \geq 20$s, the pure least-squares algorithm starts to face issues. In figure \ref{fig:varying-mass-bad} we see how the mass estimate struggles to climb up to where it should be.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth, trim={5cm 9cm 5cm 9cm, trim}]{varying-mass-bad}
\caption{Bad parameter convergence with pure LS due to varying parameters}
\label{fig:varying-mass-bad}
\end{figure}
The problem is that the old error is accumulated with pure least squares. As a result, when the parameters change, the LS algorithm still needs to fight the old error instead of work against the more recent error. If we instead add a forgetting factor to our least squares algorithm, i.e. let
\begin{equation}\begin{aligned}
\dot P &= \beta P -P \frac{\phi \phi^\top}{m^2}P,
\end{aligned}\end{equation}
we end up with a much better result as shown in figure \ref{fig:varying-mass-forget}.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth, trim={5cm 9cm 5cm 9cm, trim}]{varying-mass-forget}
\caption{Good parameter convergence using LS with forgetting factor $\beta = 0.1$ and varying parameters}
\label{fig:varying-mass-forget}
\end{figure}

\end{document}

